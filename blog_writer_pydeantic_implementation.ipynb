{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated, List, Optional, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import dotenv\n",
    "import asyncio\n",
    "from tavily import TavilyClient\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from pydantic_ai import Agent, RunContext, ModelRetry\n",
    "from dataclasses import dataclass\n",
    "from markdown_pdf import MarkdownPdf, Section\n",
    "from pydantic_ai.models import openai\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<logfire._internal.main.Logfire at 0x1748b61f850>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogfire\u001b[0m project URL: \u001b[4;36mhttps://logfire.pydantic.dev/kumarvipu1/personal-proj\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import logfire\n",
    "\n",
    "logfire.configure(token=os.getenv(\"LOGFIRE_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = openai.OpenAIModel('gpt-4o', api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlogOutput(BaseModel):\n",
    "    title: str = Field(description=\"The title of the blog post.\")\n",
    "    content: str = Field(description=\"The content of the blog post.\")\n",
    "    sources: List[str] = Field(description=\"The sources used to write the blog post.\")\n",
    "    \n",
    "@dataclass\n",
    "class BlogInput:\n",
    "    subject: str = Field(description=\"The subject of the blog post.\")\n",
    "    length: int = Field(description=\"The length of the blog post in words.\")\n",
    "    websites: List[str] = Field(description=\"The websites to scrape for more information and data for writing the blog.\")\n",
    "    image_source: List[str] = Field(description=\"The source of the image to use for the blog post.\")\n",
    "    \n",
    "\n",
    "def get_source_url(query: Annotated[str, \"The query to search for\"]) -> str:\n",
    "    \"\"\"Use this tool to get source urls for the query. Later you can use the web_scraper tool to get the content of the urls.\"\"\"\n",
    "    client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "    results = client.search(query=query, max_results=3, search_depth=\"advanced\", \n",
    "                            include_images=True, include_image_description=True)\n",
    "    print(results)\n",
    "    scores = [result['score'] for result in results['results']]\n",
    "    urls = [result['url'] for result in results['results']]\n",
    "    images = results['images'][:len(urls)]\n",
    "    return urls, images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(model=model,\n",
    "              result_type=BlogOutput,\n",
    "              deps_type=BlogInput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@agent.system_prompt\n",
    "async def get_system_prompt(ctx: RunContext[BlogInput]) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant taksed with researching and writing a blog post about {ctx.deps.subject}.\n",
    "    The blog post should be {ctx.deps.length} words long.\n",
    "    The blogpost can be in various formats like a news, research article, a tutorial, a long form article, short story, etc.\n",
    "    Structure the blogpost in relevant sections and sub-sections depending on the format based on the user pompt.\n",
    "    Think step by step.\n",
    "     \n",
    "     You have the following sources of websites to use for the blog post:\n",
    "     {ctx.deps.websites}\n",
    "     \n",
    "     You have the following source of image to use for the blog post:\n",
    "     {ctx.deps.image_source}\n",
    "     \n",
    "     You have the following tools at your disposal:\n",
    "     - web_scraper: Use this tool to get the content of the urls.\n",
    "     - download_image: Use this tool to download the image from the given source. Pass a list of names of the image as the argumenent same as the number of image urls.\n",
    "     - write_markdown_to_file: Use this tool to write the markdown to a file.\n",
    "     \n",
    "     Give the final output in the markdown format and place the images in the markdown wherever necessary.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "@agent.tool\n",
    "async def web_scraper(ctx: RunContext[BlogInput]) -> str:\n",
    "    \"\"\"Tool to scrape the content of the websites.\"\"\"\n",
    "    \n",
    "    words_per_url = ctx.deps.length // len(ctx.deps.websites)  # Distribute words evenly across URLs\n",
    "    text_data = \"\"\n",
    "    \n",
    "    for url in ctx.deps.websites:\n",
    "        loader = WebBaseLoader(url)\n",
    "        data = loader.load()\n",
    "        \n",
    "        for doc in data:\n",
    "            # Remove HTML/XML tags first\n",
    "            content = re.sub(r'<[^>]+>', '', doc.page_content)\n",
    "            \n",
    "            # Split into paragraphs\n",
    "            paragraphs = content.split('\\n')\n",
    "            clean_paragraphs = []\n",
    "            \n",
    "            for p in paragraphs:\n",
    "                # Remove special characters and normalize spaces\n",
    "                cleaned = re.sub(r'[^\\w\\s]', '', p)  # Keep only alphanumeric and spaces\n",
    "                cleaned = re.sub(r'\\s+', ' ', cleaned).strip()  # Normalize to single spaces\n",
    "                cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', cleaned)  # Remove non-English characters\n",
    "                \n",
    "                # Only keep paragraphs relevant to the query\n",
    "                if len(cleaned.split()) > 10 and cleaned:\n",
    "                    clean_paragraphs.append(cleaned)\n",
    "            \n",
    "            filtered_content = ' '.join(clean_paragraphs)  # Join all paragraphs into single text\n",
    "            final_content = ' '.join(filtered_content.split()[:words_per_url])  # Take exact number of words needed\n",
    "                \n",
    "            title = doc.metadata.get(\"title\", \"\")\n",
    "            text_data += f'{title}\\n{final_content}\\n\\n'\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "@agent.tool\n",
    "async def download_image(ctx: RunContext[BlogInput], image_names: Annotated[List[str], \"The names of the images to download\"]) -> str:\n",
    "    \"\"\"Tool to download the image from the given source.\n",
    "    Parameters:\n",
    "    - image_names: The names of the images to download.\n",
    "    \"\"\"\n",
    "    img_string = \"\"\n",
    "    for i, (image_url, image_name) in enumerate(zip(ctx.deps.image_source, image_names)):\n",
    "        try:\n",
    "            response = requests.get(image_url)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            \n",
    "            if not image_name.endswith('.jpg', '.png', '.jpeg', '.webp'):\n",
    "                image_name += '.jpg'\n",
    "            \n",
    "            with open(image_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            img_string += f\"Image {i+1} successfully downloaded and saved as {image_name}\\n\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            img_string += f\"Error downloading image {i+1}: {str(e)}\\n\"\n",
    "    \n",
    "    return img_string\n",
    "\n",
    "@agent.tool\n",
    "async def write_markdown_to_file(ctx: RunContext[None], content: Annotated[str, \"The markdown content to write\"], \n",
    "                           filename: Annotated[str, \"The name of the file (with or without .md extension)\"] = \"blog.md\") -> None:\n",
    "    \"\"\"\n",
    "    Write markdown content to a file with .md extension.\n",
    "    Parameters:\n",
    "    - content: The markdown content to write.\n",
    "    - filename: The name of the file (with or without .md extension).\n",
    "    \"\"\"\n",
    "    # Ensure filename has .md extension\n",
    "    if not filename.endswith('.md'):\n",
    "        filename += '.md'\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "        \n",
    "    pdf = MarkdownPdf()\n",
    "    pdf.add_section(Section(content, toc=False))\n",
    "    pdf.save(filename.replace('.md', '.pdf'))\n",
    "        \n",
    "    return f\"File {filename} has been created successfully. \\n the content is:\\n {content}\"\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent(user_input: str):\n",
    "    urls, images = get_source_url(user_input)\n",
    "    deps = BlogInput(subject=user_input, length=4000, websites=urls, image_source=images)\n",
    "    result = await agent.run_sync(user_input, deps=deps)\n",
    "    print(result.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the latest news on the USA and Canada situation?', 'follow_up_questions': None, 'answer': None, 'images': ['https://d.ibtimes.co.uk/en/full/1513583/canada-fire-alberta-wildfire.jpg?w=400', 'https://img-s-msn-com.akamaized.net/tenant/amp/entityid/AA1k1Ypa.img', 'https://www.vmcdn.ca/f/files/shared/site_images/national_news_2017.jpg', 'https://pbs.twimg.com/media/FgGbVGcakAA6KDz.jpg', 'https://www.ecowatch.com/wp-content/uploads/2023/07/canada-wildfire-bc.jpg'], 'results': [{'title': 'Canada and U.S. tariffs: Here are the latest updates - CTV News', 'url': 'https://www.ctvnews.ca/business/article/canada-and-the-united-states-have-announced-duelling-tariffs-heres-the-latest/', 'content': 'Trump tariffs: Canada releases full list of retaliatory targets. Live updates: Trump dealing with fallout, EU warns U.S., Canada reveals retaliation targets Live updates: Trump dealing with fallout, EU warns U.S., Canada reveals retaliation targets RECAP: Trump made good on his threat to order sweeping tariffs against Canada. Live updates: Trump dealing with fallout, EU warns U.S., Canada reveals retaliation targets Live updates: Trump dealing with fallout, EU warns U.S., Canada reveals retaliation targets Live updates: Trump dealing with fallout, EU warns U.S., Canada reveals retaliation targets ### Canada to slap 25 per cent tariff on $155B of U.S. goods after Trump initiates trade war Live updates: Trump dealing with fallout, EU warns U.S., Canada reveals retaliation targets', 'score': 0.51296157, 'raw_content': None}, {'title': 'Canada announces $155B tariff package in response to unjustified U.S ...', 'url': 'https://www.canada.ca/en/department-finance/news/2025/02/canada-announces-155b-tariff-package-in-response-to-unjustified-us-tariffs.html', 'content': 'Today, the Honourable Dominic LeBlanc, Minister of Finance and Intergovernmental Affairs, and the Honourable Mélanie Joly, Minister of Foreign Affairs, announced that the Government of Canada is moving forward with 25 per cent tariffs on $155 billion worth of goods in response to the unjustified and unreasonable tariffs imposed by the United States (U.S.) on Canadian goods. Today, the Honourable Dominic LeBlanc, Minister of Finance and Intergovernmental Affairs, and the Honourable Mélanie Joly, Minister of Foreign Affairs, announced that the Government of Canada is moving forward with 25 per cent tariffs on $155 billion worth of goods in response to the unjustified and unreasonable tariffs imposed by the United States (U.S.) on Canadian goods.', 'score': 0.42821008, 'raw_content': None}, {'title': 'Trudeau says Canada would respond to tariffs, warns of tough times to ...', 'url': 'https://www.reuters.com/world/americas/trudeau-says-canada-would-respond-immediately-any-us-tariffs-2025-01-31/', 'content': 'Canada will respond immediately and forcefully if the United States goes ahead with a threat to impose tariffs, Prime Minister Justin Trudeau said on Friday, warning Canadians that they could be', 'score': 0.3442896, 'raw_content': None}], 'response_time': 3.2}\n"
     ]
    }
   ],
   "source": [
    "output = await run_agent(\"What is the latest news on the USA and Canada situation?\")\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
